\chapter{Stellar Activity Modelling in Radial Velocity Time Series}
As discussed in Sect.~\ref{sect:activity}, there exists a multitude of physical
processes ongoing within the photospheres and chromospheres of active stars
which produce observable signatures with a variety of amplitudes and timescales
(see Table~\ref{table:activity}). The subsequent sections discuss a variety of
techniques which have been used to model and consequently mitigate the effects of
stellar activity in RV time series.

\section{An Overview of Techniques for Stellar Activity Mitigation}
\label{sect:methods}

\subsection{Stellar Activity as a Scalar Parameter}
Back in the day when the first giant exoplanets where being discovered with RVs,
typically treatment of stellar activity was implemented. The reason for this being
that the quality of the datasets at that time were insufficient to resolve the
temporal structure of RV activity for any but the most active stars. Many
observers however did report the root-mean-square (rms) of their RV residuals
following the removal of their best-fit planet model
\citep[e.g.][]{mayor95,butler96}. In
many cases, the residual rms was seen to exceed the characteristic RV measurement
uncertainty of the dataset and thus alluded to the presence of an additional
jitter signal which may or may not vary significantly with time. \\

In many of the following RV planet searches, the apparent jitter was characterized
by an additive scalar $s$. The free parameter $s$ was used to characterize the
level of RV jitter as it was added in quadrature to the RV measurement
uncertainties when evaluating the objective function during any analysis
equivalent or analogous to a $\chi^2$-minimization routine. 


\subsection{Correlations with Activity Indicators}
Stellar RV observations are known to be affected by both planetary companions as
well as by stellar activity. Disentangling those signals in RV time series
therefore benefits significantly from ancillary time series which are sensitive
to stellar activity only \citep{boisse09}.
The classical implementation of de-correlation by
an activity indicator is to derive time series of one or many spectroscopic
activity indicators whose sampling is contemporaneous with the RVs and then
fitting an often linear relation between those datasets (Fig.~\ref{fig:corr}).
The relation, when fitted simultaneously with planetary solutions, allows the RVs
to be de-correlated jointly with the measurement of the planetary parameters.
This technique has been shown to be effective when the stellar rotation period
\prot{} is well constrained, the planetary orbital period is distinct from
\prot{,} the amplitude of the planetary signal exceeds that of the activity signal
by $\sim 30$\%, and the stellar rotation period is well sampled over multiple
cycles \citep{boisse11}. \\

\begin{figure}
  \centering
  %\includegraphics[width=.6\textwidth]{figures/vspan_rv.png}
  \caption{The correlation of the \vspan{} activity indicator with the RVs for the
    active Sun-like star HD 17051 from HARPS spectra. The solid line depicts the
    best-fit linear relation from least squares fitting. The fitted relation is
    used to de-correlate the RVs for the effects of stellar activity as probed by
    the \vspan{} time series. \citep[Image credit:][]{boisse11}.}
  \label{fig:corr}
\end{figure}

There exists a number of activity indicators which can be derived from the stellar
spectra. There definitions and physical motivations are summarized below.

\caii{} H \& K lines: 
For optical spectra with access to the \caii{} H and K lines
centered on 3968.47 \AA{} and 3933.66 \AA{,} the Mt. Wilson S-index is defined as

\begin{equation}
  \text{S-index} = \frac{\Psi_H + \Psi_K}{\Psi_B + \Psi_V}
\end{equation}

\noindent where $\Psi_H$ and $\Psi_K$ represent the narrowband
($\sim 1.1$ \AA{} wide) fluxes in the cores of the H and K lines of the \caii{}
doublet. The index is normalized by total flux in the $B$ and $V$ continuum bands
which are 20 \AA{} wide broad bands centered on 3900 and 4000 \AA{} respectively.

H$\alpha$: ...
Similar emission line features may also be used as activity indicators depending
on the accessible wavelengths of the spectrograph (e.g. \hei{,} \nai{,} etc).

Vspan:
\citep{queloz01}

FWHM:

BIS:


\citep{boisse09,boisse11}

\subsection{Photometric Modelling: the FF' Method }
\citep{aigrain12}

\subsection{Pre-whitening}
\citep{queloz09}

\subsection{Physical Models of Active Regions}
\citep{dumusque14}

\subsection{Deterministic Model Fitting}
\citep{sarkis18}

\subsection{Spectral Feature Decomposition}
\citep{davis17}


\section{Gaussian Process Regression for Non-Parametric Activity Modelling} \label{sect:gp}
Many of the shortcomings of the activity mitigation techniques discussed in the
previous section are based on their incompleteness and their inability to
self-consistently characterize model uncertainties. For example, correlations with
activity-sensitive time series are often incomplete because the chosen indicator
is not sensitive to also activity sources present in the RVs and often result in
large RV residuals after de-correlation. Similarly, deterministic activity models
lack sensitivity to stochastic changes in activity levels due to variations in
the configuration of active regions between adjacent rotation cycles or over
significant subsets of long-term magnetic cycles. \\

Gaussian process regression models represent one such tool which aims to overcome
these shortcomings by jointly modelling planets and activity in a self-consistent
manner. This technique originally pioneered on active Sun-like stars
\citep{haywood14,rajpaul15} has been shown to reconcile disparate solutions
between observations from multiple spectrographs \citep{rajpaul17,cloutier18b},
to reconcile RV solutions for planetary systems whose favoured models are
ambiguous \citep{rajpaul17,cloutier18b}, and to disentangle neighbouring periodic
signals from planets, stellar rotation, and/or from window functions
\citep{rajpaul16,cloutier17b}.
In the following sections I will give an overview
of what Gaussian processes are and how I implement their formalism in the context
of modelling stellar activity in RV time series.

\subsection{The one dimensional Gaussian distribution}
A Gaussian process (GP) is defined as ``a collection of random variables, any
finite number of which have a joint Gaussian distribution''. In other words,
any random process\footnote{A random process is any collection of random
  variables indexed by an independent variable such as time.} for which all
finite subsets have a multivariate Gaussian distribution, is a Gaussian process.

In order to develop a visual intuition as to what this means, let us a first
consider a Gaussian random variable in one dimension. Imagine some random
process that draws a single value of a random variable $X$ in each realization of
an experiment. As an astronomer with a life
outside of astronomy, and furthermore as an astronomer who spends many of his
off-work hours being pelted with 170 gram disks of vulcanized rubber, my favourite
Gaussian random variable has to be the `goals against average' statistic:

\begin{equation}
  X = GAA \equiv \frac{\text{total number of goals against}}{\text{total number of games played}}.
\end{equation}

\noindent As a Gaussian random variable, the results of repeated experiments (i.e.
repeated draws from the empirical $GAA$ distribution for various goaltenders) can
be expressed as a probability density function of the form

\begin{equation}
  \mathcal{N}(X|\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp{\left(
    -\frac{(X-\mu)^2}{2\sigma^2} \right)},
  \label{eq:gauss}
\end{equation}

\noindent which is parameterized by the mean value of $X$, $\mu$, and the variance
of $X$, $\sigma^2$. As shown in Fig.~\ref{fig:gaa1d}, the empirical distribution
of the $GAA$ for goaltenders in
the National Hockey League (NHL) over the past five seasons closely follows a
Gaussian distribution with a mean of 2.64 and a standard deviation of 0.40 goals
against per game. Hence my favourite Gaussian random variable. \\

\begin{figure}
  \centering
  %\includegraphics[width=.8\textwidth]{figures/GAA1D.png}
  \caption{The distribution of $GAA$ for NHL goaltenders over the past five
    seasons as an example of a Gaussian random variable. The solid line depicts
    the Gaussian probability density function with mean and standard
    deviation 2.64 and 0.40 goals against per game.}
  \label{fig:gaa1d}
\end{figure}

Before a new season begins, we can write down a prior distribution on the $GAA$
based on our prior knowledge have obtained from previous NHL seasons; i.e.
$\mathcal{N}(GAA|2.64,0.40)$. This distribution has a unique expection value and
variance describing our what we expect a given goaltender's $GAA$ to be in an
up-coming season, and as crucially, how precise that value is.

\subsection{The two dimensional Gaussian distribution}
Of course we can introduce a second Gaussian random variable and extend the 
concept of the one dimensional Gaussian distribution to two dimensions. We must
first recast the expression for the Gaussian probability density function in $k$
dimensions where $k=2$ in this scenario. Rewriting Eq.~\ref{eq:gauss} in $k$
dimensions is achieved by replacing the scalar mean and variance with a mean
$k$-vector $\boldsymbol{\mu}$ and a $k \times k$ covariance matrix $\mathbf{K}$
respectively. The updated expression for the so-called multivariate Gaussian
distribution is

\begin{equation}
  p(\mathbf{X}|\boldsymbol{\mu},\mathbf{K}) = \frac{1}{\sqrt{(2\pi)^k
      |\mathbf{K}|}} \exp{\left( -\frac{1}{2} (\mathbf{X} -
    \boldsymbol{\mu})^{\text{T}} \mathbf{K}^{-1} (\mathbf{X}-\boldsymbol{\mu})
    \right)}.
\end{equation}

\noindent The $k$-vector $\boldsymbol{\mu}$ represents the mean value of each
random variable in $\mathbf{X}=(X_1,\dots,X_k)^{\text{T}}$.
The elements of the covariance matrix
$\mathbf{K}$ represent the covariances of between each pair of random variables
with the diagonal elements representing the covariance of a random variable
with itself, or equivalently, the variance of that random variable.
The covariance between two random variables $X_1$ and $X_2$ is computed in terms
of their expected values E[$X_1$] and E[$X_2$] via

\begin{align}
  \text{cov}(X_1X_2) &= \text{E}[(X_1-\text{E}[X_1]) - (X_2-\text{E}[X_2])] \\
  &= \text{E}[X_1X_2] - \text{E}[X_1]\text{E}[X_2].
  \label{eq:coveq}
\end{align}

\noindent Note that the expected value of a Gaussian random variable is simply
its mean. \\

Consider the special case when the two Gaussian random variables under
consideration $X_1$ and $X_2$ are uncorrelated (i.e. cov$(X_1X_2)=0$). A quick
example of this is to let $X_1=GAA$ and sample $X_2$ from a Gaussian distribution
with zero mean and unit variance. The nature of sampling $X_2$ in this way ensures
that these variables are indeed uncorrelated. The corresponding covariance matrix

\begin{equation}
  \mathbf{K}(GAA,X_2) =
  \begin{bmatrix}
    0.15 & 0.00 \\
    0.00 & 0.98
  \end{bmatrix}
  \label{eq:Kuncorr}
\end{equation}

\noindent calculated using Eq.~\ref{eq:correq} is diagonal and confirms that
$X_1$ and $X_2$ are uncorrelated. This fact can be seen in the joint $X_1X_2$
distribution in Fig.~\ref{fig:uncorr2d}.
Because the varaibles $X_1$ and $X_2$ are uncorrelated in this example, the
measurement of a value of $X_1$ does nothing to inform us of its corresponding
$X_2$ value. That is that the prior distribution
$p(\mathbf{X}|\boldsymbol{\mu},\mathbf{K})$ is uninformative with regards to
$X_2$ when the corresponding value of $X_1$ is known. \\

\begin{figure}
  \centering
  \includegraphics[width=.9\textwidth]{figures/uncorr_2d.png}
  \caption{The one and two dimensional distributions of the Gaussian random
    variables $GAA$ and draws from the standard normal disribution. The solid
    lines overlaid on their joint distribution represent the 1, 2, and 3$\sigma$
    contours. The covariance matrix (Eq.~\ref{eq:Kuncorr}) has off-diagonal
    elements equal to zero indiciating that the two variables are uncorrelated;
    a property which is largely discernable from their joint distribution.}
  \label{fig:uncorr2d}
\end{figure}

But what if we consider an alternate Gaussian random variable and set
$X_2 = SV$\% where

\begin{equation}
  X_2 = SV\% \equiv \frac{\text{total number of save made}}{\text{total number of shots faced}}
\end{equation}

\noindent is a measure of a goaltender's save percentage. In this scenario one
might expect the variables $GAA$ and $SV$\% to have some degree of correlation
as a good goaltender who boasts a low $GAA$ probably does so because of their
high $SV$\%. Indeed the covariance matrix of the $GAA$ and $SV$\% for NHL
goaltenders over the past five seasons is

\begin{equation}
  \mathbf{K}(GAA,SV\%) =
  \begin{bmatrix}
    0.1496 & -0.0037 \\
    -0.0037 & 0.0001
  \end{bmatrix}
  \label{eq:Kcorr}
\end{equation}

\noindent and has non-zero off-diagonal elements. Furthermore, the off-diagonal
elements are negative which is indicative of the anti-correlation between the
$GAA$ and the $SV$\%. This strong correlation is easily visualized in their joint
distribution in Fig.~\ref{fig:corr2d}. \\

The fact that the two variables are
correlated implies that knowledge of one variable's value provide some information
regarding the value of the second. This demonstrated in Fig.~\ref{fig:corr2d}
wherein we measure the value of the $GAA$ of the Toronto Maple Leafs goaltender
this year to be 2.57 goals per game. Not bad. Not great, but not bad. By measuring
this value we can establish a posterior distribution on Freddie's corresponding
$SV$\% given his $GAA$: $p(SV\%|GAA=2.57)$. Because the variables are dependent
on each other according to Eq.~\ref{eq:Kcorr}, this posterior distribution is
narrower (i.e. more precise) than the full $SV$\% distribution. Indeed this is
evidenced in the lower right panel of Fig.~\ref{fig:Kcorr} which compares the
two $SV$\% distributions and reveals that the dispersion in $p(SV\%|GAA=2.57)$
has approximately half of the dispersion as the full $SV$\% distribution. \\

\begin{figure}
  \centering
  %\includegraphics[width=.9\textwidth]{figures/corr_2D_HARTpost.png}
  \caption{The one and two dimensional distributions of the Gaussian random
    variables $GAA$ and $SV$\%.
    The covariance matrix (Eq.~\ref{eq:Kcorr}) has negative off-diagonal
    elements describing the degree of anti-correlation between the two
    variables; a property which is largely discernable from their joint
    distribution as high values of the $SV$\% tend to correspond to a lower
    $GAA$. The correlation can be used to inform the value of $SV$\% given a
    measured value of $GAA$. This is demonstrated as the measured $GAA$ 
    for the Toronto Maple Leaf's goaltender (vertical dashed line) provides some
    additional information on the correpsonding $SV$\% whose posterior given the
    measured $GAA$ is more tightly constrained than the $SV$\% distribution for
    the entire NHL.}
  \label{fig:corr2d}
\end{figure}

The notion that the value of an unseen variable can be informed by the measurement
of another, if those variables are correlated, can have huge implications for
predictive models.


\section{Point-form Thesis: Stellar Activity Modelling in Radial Velocity
  Time Series}
\begin{itemize}
\renewcommand\labelitemi{--}
\item~\ref{sect:methods} \textbf{An Overview of Techniques for Stellar Activity
Mitigation}: hi \\
\item~\ref{sect:gp} \textbf{Gaussian Process Regression for Non-Parametric
  Activity Modelling}:
\end{itemize}
